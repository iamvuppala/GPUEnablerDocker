//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21313570
// Cuda compilation tools, release 8.0, V8.0.53
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_35
.address_size 64

	// .globl	_Z3mapiPdS_S_S_i

.visible .entry _Z3mapiPdS_S_S_i(
	.param .u32 _Z3mapiPdS_S_S_i_param_0,
	.param .u64 _Z3mapiPdS_S_S_i_param_1,
	.param .u64 _Z3mapiPdS_S_S_i_param_2,
	.param .u64 _Z3mapiPdS_S_S_i_param_3,
	.param .u64 _Z3mapiPdS_S_S_i_param_4,
	.param .u32 _Z3mapiPdS_S_S_i_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<36>;
	.reg .f64 	%fd<57>;
	.reg .b64 	%rd<32>;


	ld.param.u32 	%r13, [_Z3mapiPdS_S_S_i_param_0];
	ld.param.u64 	%rd18, [_Z3mapiPdS_S_S_i_param_1];
	ld.param.u64 	%rd17, [_Z3mapiPdS_S_S_i_param_2];
	ld.param.u64 	%rd19, [_Z3mapiPdS_S_S_i_param_3];
	ld.param.u64 	%rd20, [_Z3mapiPdS_S_S_i_param_4];
	ld.param.u32 	%r12, [_Z3mapiPdS_S_S_i_param_5];
	cvta.to.global.u64 	%rd1, %rd20;
	cvta.to.global.u64 	%rd2, %rd18;
	cvta.to.global.u64 	%rd3, %rd19;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r4, %r13;
	@%p1 bra 	BB0_10;

	ld.global.f64 	%fd55, [%rd3];
	setp.lt.s32	%p2, %r12, 1;
	@%p2 bra 	BB0_4;

	add.s32 	%r15, %r12, -1;
	add.s64 	%rd29, %rd3, 8;
	mul.lo.s32 	%r17, %r4, %r15;
	mul.wide.s32 	%rd21, %r17, 8;
	add.s64 	%rd28, %rd2, %rd21;
	mov.u32 	%r34, 0;

BB0_3:
	ld.global.f64 	%fd10, [%rd29];
	ld.global.f64 	%fd11, [%rd28];
	fma.rn.f64 	%fd55, %fd11, %fd10, %fd55;
	add.s64 	%rd29, %rd29, 8;
	add.s64 	%rd28, %rd28, 8;
	add.s32 	%r34, %r34, 1;
	setp.lt.s32	%p3, %r34, %r12;
	@%p3 bra 	BB0_3;

BB0_4:
	neg.f64 	%fd12, %fd55;
	mov.f64 	%fd13, 0d4338000000000000;
	mov.f64 	%fd14, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd15, %fd12, %fd14, %fd13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd15;
	}
	mov.f64 	%fd16, 0dC338000000000000;
	add.rn.f64 	%fd17, %fd15, %fd16;
	mov.f64 	%fd18, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd19, %fd17, %fd18, %fd12;
	mov.f64 	%fd20, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd21, %fd17, %fd20, %fd19;
	mov.f64 	%fd22, 0d3E928AF3FCA213EA;
	mov.f64 	%fd23, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd24, %fd23, %fd21, %fd22;
	mov.f64 	%fd25, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd26, %fd24, %fd21, %fd25;
	mov.f64 	%fd27, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd28, %fd26, %fd21, %fd27;
	mov.f64 	%fd29, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd30, %fd28, %fd21, %fd29;
	mov.f64 	%fd31, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd32, %fd30, %fd21, %fd31;
	mov.f64 	%fd33, 0d3F81111111122322;
	fma.rn.f64 	%fd34, %fd32, %fd21, %fd33;
	mov.f64 	%fd35, 0d3FA55555555502A1;
	fma.rn.f64 	%fd36, %fd34, %fd21, %fd35;
	mov.f64 	%fd37, 0d3FC5555555555511;
	fma.rn.f64 	%fd38, %fd36, %fd21, %fd37;
	mov.f64 	%fd39, 0d3FE000000000000B;
	fma.rn.f64 	%fd40, %fd38, %fd21, %fd39;
	mov.f64 	%fd41, 0d3FF0000000000000;
	fma.rn.f64 	%fd42, %fd40, %fd21, %fd41;
	fma.rn.f64 	%fd43, %fd42, %fd21, %fd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r8, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd43;
	}
	shl.b32 	%r18, %r7, 20;
	add.s32 	%r19, %r9, %r18;
	mov.b64 	%fd56, {%r8, %r19};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd12;
	}
	mov.b32 	 %f2, %r20;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB0_7;

	setp.gt.f64	%p5, %fd55, 0d8000000000000000;
	mov.f64 	%fd44, 0d7FF0000000000000;
	sub.f64 	%fd45, %fd44, %fd55;
	selp.f64	%fd56, 0d0000000000000000, %fd45, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB0_7;

	shr.u32 	%r21, %r7, 31;
	add.s32 	%r22, %r7, %r21;
	shr.s32 	%r23, %r22, 1;
	shl.b32 	%r24, %r23, 20;
	add.s32 	%r25, %r24, %r9;
	mov.b64 	%fd46, {%r8, %r25};
	sub.s32 	%r26, %r7, %r23;
	shl.b32 	%r27, %r26, 20;
	add.s32 	%r28, %r27, 1072693248;
	mov.u32 	%r29, 0;
	mov.b64 	%fd47, {%r29, %r28};
	mul.f64 	%fd56, %fd46, %fd47;

BB0_7:
	cvta.to.global.u64 	%rd22, %rd17;
	add.f64 	%fd48, %fd56, 0d3FF0000000000000;
	rcp.rn.f64 	%fd9, %fd48;
	mul.wide.s32 	%rd23, %r4, 8;
	add.s64 	%rd10, %rd22, %rd23;
	ld.global.f64 	%fd49, [%rd10];
	sub.f64 	%fd50, %fd9, %fd49;
	mul.lo.s32 	%r30, %r4, %r12;
	mul.wide.s32 	%rd24, %r30, 8;
	add.s64 	%rd25, %rd1, %rd24;
	st.global.f64 	[%rd25], %fd50;
	setp.lt.s32	%p7, %r12, 2;
	@%p7 bra 	BB0_10;

	mul.lo.s32 	%r33, %r12, %r4;
	mul.wide.s32 	%rd26, %r33, 8;
	add.s64 	%rd27, %rd26, 8;
	add.s64 	%rd31, %rd1, %rd27;
	add.s64 	%rd30, %rd2, %rd27;
	mov.u32 	%r35, 1;

BB0_9:
	ld.global.f64 	%fd51, [%rd10];
	sub.f64 	%fd52, %fd9, %fd51;
	ld.global.f64 	%fd53, [%rd30];
	mul.f64 	%fd54, %fd52, %fd53;
	st.global.f64 	[%rd31], %fd54;
	add.s64 	%rd31, %rd31, 8;
	add.s64 	%rd30, %rd30, 8;
	add.s32 	%r35, %r35, 1;
	setp.lt.s32	%p8, %r35, %r12;
	@%p8 bra 	BB0_9;

BB0_10:
	ret;
}

	// .globl	reducegrad
.visible .entry reducegrad(
	.param .u32 reducegrad_param_0,
	.param .u64 reducegrad_param_1,
	.param .u64 reducegrad_param_2,
	.param .u32 reducegrad_param_3
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<373>;
	.reg .f64 	%fd<99>;
	.reg .b64 	%rd<66>;


	ld.param.u32 	%r16, [reducegrad_param_0];
	ld.param.u64 	%rd35, [reducegrad_param_1];
	ld.param.u64 	%rd36, [reducegrad_param_2];
	ld.param.u32 	%r17, [reducegrad_param_3];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r18, %r1, %r2;
	setp.ge.s32	%p1, %r3, %r16;
	@%p1 bra 	BB1_24;

	cvt.s64.s32	%rd3, %r17;
	cvt.s64.s32	%rd4, %r16;
	mov.u64 	%rd63, 0;
	setp.lt.s32	%p2, %r17, 4;
	@%p2 bra 	BB1_16;

	cvt.u64.u32	%rd5, %r3;
	and.b32  	%r4, %r2, 31;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r20, %r19, %r1;
	cvt.u64.u32	%rd6, %r20;
	add.s32 	%r21, %r3, 16;
	shr.s32 	%r22, %r21, 31;
	shr.u32 	%r23, %r22, 27;
	add.s32 	%r24, %r21, %r23;
	and.b32  	%r25, %r24, -32;
	sub.s32 	%r5, %r21, %r25;
	add.s32 	%r26, %r3, 8;
	shr.s32 	%r27, %r26, 31;
	shr.u32 	%r28, %r27, 27;
	add.s32 	%r29, %r26, %r28;
	and.b32  	%r30, %r29, -32;
	sub.s32 	%r6, %r26, %r30;
	add.s32 	%r31, %r3, 4;
	shr.s32 	%r32, %r31, 31;
	shr.u32 	%r33, %r32, 27;
	add.s32 	%r34, %r31, %r33;
	and.b32  	%r35, %r34, -32;
	sub.s32 	%r7, %r31, %r35;
	add.s32 	%r36, %r3, 2;
	shr.s32 	%r37, %r36, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r36, %r38;
	and.b32  	%r40, %r39, -32;
	sub.s32 	%r8, %r36, %r40;
	add.s32 	%r41, %r3, 1;
	shr.s32 	%r42, %r41, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r41, %r43;
	and.b32  	%r45, %r44, -32;
	sub.s32 	%r9, %r41, %r45;
	mov.u64 	%rd63, 0;

BB1_3:
	mov.f64 	%fd97, 0d0000000000000000;
	mov.f64 	%fd96, %fd97;
	mov.f64 	%fd95, %fd97;
	mov.f64 	%fd94, %fd97;
	setp.ge.s64	%p3, %rd5, %rd4;
	@%p3 bra 	BB1_6;

	mov.f64 	%fd97, 0d0000000000000000;
	mov.f64 	%fd96, %fd97;
	mov.f64 	%fd95, %fd97;
	mov.f64 	%fd94, %fd97;
	mov.u64 	%rd58, %rd5;

BB1_5:
	mov.u64 	%rd8, %rd58;
	mul.lo.s64 	%rd39, %rd8, %rd3;
	add.s64 	%rd40, %rd39, %rd63;
	shl.b64 	%rd41, %rd40, 3;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.f64 	%fd28, [%rd42];
	add.f64 	%fd94, %fd94, %fd28;
	ld.global.f64 	%fd29, [%rd42+8];
	add.f64 	%fd95, %fd95, %fd29;
	ld.global.f64 	%fd30, [%rd42+16];
	add.f64 	%fd96, %fd96, %fd30;
	ld.global.f64 	%fd31, [%rd42+24];
	add.f64 	%fd97, %fd97, %fd31;
	add.s64 	%rd9, %rd6, %rd8;
	setp.lt.s64	%p4, %rd9, %rd4;
	mov.u64 	%rd58, %rd9;
	@%p4 bra 	BB1_5;

BB1_6:
	// inline asm
	mov.b64 {%r46,%r47}, %fd94;
	// inline asm
	// inline asm
	mov.b64 {%r48,%r49}, %fd95;
	// inline asm
	// inline asm
	mov.b64 {%r50,%r51}, %fd96;
	// inline asm
	// inline asm
	mov.b64 {%r52,%r53}, %fd97;
	// inline asm
	mov.u32 	%r277, 31;
	// inline asm
	shfl.idx.b32 %r54, %r46, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r58, %r47, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r62, %r48, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r66, %r49, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r70, %r50, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r74, %r51, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r78, %r52, %r5, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r82, %r53, %r5, %r277;
	// inline asm
	// inline asm
	mov.b64 %fd36, {%r54,%r58};
	// inline asm
	// inline asm
	mov.b64 %fd37, {%r62,%r66};
	// inline asm
	// inline asm
	mov.b64 %fd38, {%r70,%r74};
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r78,%r82};
	// inline asm
	add.f64 	%fd40, %fd94, %fd36;
	add.f64 	%fd41, %fd95, %fd37;
	add.f64 	%fd42, %fd96, %fd38;
	add.f64 	%fd43, %fd97, %fd39;
	// inline asm
	mov.b64 {%r94,%r95}, %fd40;
	// inline asm
	// inline asm
	mov.b64 {%r96,%r97}, %fd41;
	// inline asm
	// inline asm
	mov.b64 {%r98,%r99}, %fd42;
	// inline asm
	// inline asm
	mov.b64 {%r100,%r101}, %fd43;
	// inline asm
	// inline asm
	shfl.idx.b32 %r102, %r94, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r106, %r95, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r110, %r96, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r114, %r97, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r118, %r98, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r122, %r99, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r126, %r100, %r6, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r130, %r101, %r6, %r277;
	// inline asm
	// inline asm
	mov.b64 %fd44, {%r102,%r106};
	// inline asm
	// inline asm
	mov.b64 %fd45, {%r110,%r114};
	// inline asm
	// inline asm
	mov.b64 %fd46, {%r118,%r122};
	// inline asm
	// inline asm
	mov.b64 %fd47, {%r126,%r130};
	// inline asm
	add.f64 	%fd48, %fd40, %fd44;
	add.f64 	%fd49, %fd41, %fd45;
	add.f64 	%fd50, %fd42, %fd46;
	add.f64 	%fd51, %fd43, %fd47;
	// inline asm
	mov.b64 {%r142,%r143}, %fd48;
	// inline asm
	// inline asm
	mov.b64 {%r144,%r145}, %fd49;
	// inline asm
	// inline asm
	mov.b64 {%r146,%r147}, %fd50;
	// inline asm
	// inline asm
	mov.b64 {%r148,%r149}, %fd51;
	// inline asm
	// inline asm
	shfl.idx.b32 %r150, %r142, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r154, %r143, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r158, %r144, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r162, %r145, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r166, %r146, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r170, %r147, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r174, %r148, %r7, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r178, %r149, %r7, %r277;
	// inline asm
	// inline asm
	mov.b64 %fd52, {%r150,%r154};
	// inline asm
	// inline asm
	mov.b64 %fd53, {%r158,%r162};
	// inline asm
	// inline asm
	mov.b64 %fd54, {%r166,%r170};
	// inline asm
	// inline asm
	mov.b64 %fd55, {%r174,%r178};
	// inline asm
	add.f64 	%fd56, %fd48, %fd52;
	add.f64 	%fd57, %fd49, %fd53;
	add.f64 	%fd58, %fd50, %fd54;
	add.f64 	%fd59, %fd51, %fd55;
	// inline asm
	mov.b64 {%r190,%r191}, %fd56;
	// inline asm
	// inline asm
	mov.b64 {%r192,%r193}, %fd57;
	// inline asm
	// inline asm
	mov.b64 {%r194,%r195}, %fd58;
	// inline asm
	// inline asm
	mov.b64 {%r196,%r197}, %fd59;
	// inline asm
	// inline asm
	shfl.idx.b32 %r198, %r190, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r202, %r191, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r206, %r192, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r210, %r193, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r214, %r194, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r218, %r195, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r222, %r196, %r8, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r226, %r197, %r8, %r277;
	// inline asm
	// inline asm
	mov.b64 %fd60, {%r198,%r202};
	// inline asm
	// inline asm
	mov.b64 %fd61, {%r206,%r210};
	// inline asm
	// inline asm
	mov.b64 %fd62, {%r214,%r218};
	// inline asm
	// inline asm
	mov.b64 %fd63, {%r222,%r226};
	// inline asm
	add.f64 	%fd64, %fd56, %fd60;
	add.f64 	%fd65, %fd57, %fd61;
	add.f64 	%fd66, %fd58, %fd62;
	add.f64 	%fd67, %fd59, %fd63;
	// inline asm
	mov.b64 {%r238,%r239}, %fd64;
	// inline asm
	// inline asm
	mov.b64 {%r240,%r241}, %fd65;
	// inline asm
	// inline asm
	mov.b64 {%r242,%r243}, %fd66;
	// inline asm
	// inline asm
	mov.b64 {%r244,%r245}, %fd67;
	// inline asm
	// inline asm
	shfl.idx.b32 %r246, %r238, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r250, %r239, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r254, %r240, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r258, %r241, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r262, %r242, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r266, %r243, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r270, %r244, %r9, %r277;
	// inline asm
	// inline asm
	shfl.idx.b32 %r274, %r245, %r9, %r277;
	// inline asm
	// inline asm
	mov.b64 %fd68, {%r246,%r250};
	// inline asm
	// inline asm
	mov.b64 %fd69, {%r254,%r258};
	// inline asm
	// inline asm
	mov.b64 %fd70, {%r262,%r266};
	// inline asm
	// inline asm
	mov.b64 %fd71, {%r270,%r274};
	// inline asm
	add.f64 	%fd13, %fd64, %fd68;
	add.f64 	%fd14, %fd65, %fd69;
	add.f64 	%fd15, %fd66, %fd70;
	add.f64 	%fd16, %fd67, %fd71;
	setp.ne.s32	%p5, %r4, 0;
	@%p5 bra 	BB1_15;

	shl.b64 	%rd43, %rd63, 3;
	add.s64 	%rd10, %rd1, %rd43;
	ld.global.u64 	%rd59, [%rd10];

BB1_8:
	mov.u64 	%rd12, %rd59;
	mov.b64 	 %fd72, %rd12;
	add.f64 	%fd73, %fd13, %fd72;
	mov.b64 	 %rd44, %fd73;
	atom.global.cas.b64 	%rd59, [%rd10], %rd12, %rd44;
	setp.ne.s64	%p6, %rd12, %rd59;
	@%p6 bra 	BB1_8;

	ld.global.u64 	%rd60, [%rd10+8];

BB1_10:
	mov.u64 	%rd15, %rd60;
	add.s64 	%rd45, %rd10, 8;
	mov.b64 	 %fd74, %rd15;
	add.f64 	%fd75, %fd14, %fd74;
	mov.b64 	 %rd46, %fd75;
	atom.global.cas.b64 	%rd60, [%rd45], %rd15, %rd46;
	setp.ne.s64	%p7, %rd15, %rd60;
	@%p7 bra 	BB1_10;

	ld.global.u64 	%rd61, [%rd10+16];

BB1_12:
	mov.u64 	%rd18, %rd61;
	add.s64 	%rd47, %rd10, 16;
	mov.b64 	 %fd76, %rd18;
	add.f64 	%fd77, %fd15, %fd76;
	mov.b64 	 %rd48, %fd77;
	atom.global.cas.b64 	%rd61, [%rd47], %rd18, %rd48;
	setp.ne.s64	%p8, %rd18, %rd61;
	@%p8 bra 	BB1_12;

	ld.global.u64 	%rd62, [%rd10+24];

BB1_14:
	mov.u64 	%rd21, %rd62;
	add.s64 	%rd49, %rd10, 24;
	mov.b64 	 %fd78, %rd21;
	add.f64 	%fd79, %fd16, %fd78;
	mov.b64 	 %rd50, %fd79;
	atom.global.cas.b64 	%rd62, [%rd49], %rd21, %rd50;
	setp.ne.s64	%p9, %rd21, %rd62;
	@%p9 bra 	BB1_14;

BB1_15:
	add.s64 	%rd63, %rd63, 4;
	sub.s64 	%rd51, %rd3, %rd63;
	setp.gt.s64	%p10, %rd51, 3;
	@%p10 bra 	BB1_3;

BB1_16:
	setp.ge.s64	%p11, %rd63, %rd3;
	@%p11 bra 	BB1_24;

	cvt.s64.s32	%rd25, %r3;
	mov.u32 	%r286, %nctaid.x;
	mul.lo.s32 	%r287, %r286, %r1;
	cvt.u64.u32	%rd26, %r287;
	and.b32  	%r10, %r2, 31;
	add.s32 	%r288, %r3, 16;
	shr.s32 	%r289, %r288, 31;
	shr.u32 	%r290, %r289, 27;
	add.s32 	%r291, %r288, %r290;
	and.b32  	%r292, %r291, -32;
	sub.s32 	%r11, %r288, %r292;
	add.s32 	%r293, %r3, 8;
	shr.s32 	%r294, %r293, 31;
	shr.u32 	%r295, %r294, 27;
	add.s32 	%r296, %r293, %r295;
	and.b32  	%r297, %r296, -32;
	sub.s32 	%r12, %r293, %r297;
	add.s32 	%r298, %r3, 4;
	shr.s32 	%r299, %r298, 31;
	shr.u32 	%r300, %r299, 27;
	add.s32 	%r301, %r298, %r300;
	and.b32  	%r302, %r301, -32;
	sub.s32 	%r13, %r298, %r302;
	add.s32 	%r303, %r3, 2;
	shr.s32 	%r304, %r303, 31;
	shr.u32 	%r305, %r304, 27;
	add.s32 	%r306, %r303, %r305;
	and.b32  	%r307, %r306, -32;
	sub.s32 	%r14, %r303, %r307;
	add.s32 	%r308, %r3, 1;
	shr.s32 	%r309, %r308, 31;
	shr.u32 	%r310, %r309, 27;
	add.s32 	%r311, %r308, %r310;
	and.b32  	%r312, %r311, -32;
	sub.s32 	%r15, %r308, %r312;

BB1_18:
	shl.b64 	%rd52, %rd63, 3;
	add.s64 	%rd28, %rd1, %rd52;
	mov.f64 	%fd98, 0d0000000000000000;
	mov.u64 	%rd64, %rd25;

BB1_19:
	mov.u64 	%rd29, %rd64;
	mul.lo.s64 	%rd53, %rd29, %rd3;
	add.s64 	%rd54, %rd53, %rd63;
	shl.b64 	%rd55, %rd54, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd81, [%rd56];
	add.f64 	%fd98, %fd98, %fd81;
	add.s64 	%rd30, %rd26, %rd29;
	setp.lt.s64	%p12, %rd30, %rd4;
	mov.u64 	%rd64, %rd30;
	@%p12 bra 	BB1_19;

	// inline asm
	mov.b64 {%r313,%r314}, %fd98;
	// inline asm
	mov.u32 	%r370, 31;
	// inline asm
	shfl.idx.b32 %r315, %r313, %r11, %r370;
	// inline asm
	// inline asm
	shfl.idx.b32 %r319, %r314, %r11, %r370;
	// inline asm
	// inline asm
	mov.b64 %fd83, {%r315,%r319};
	// inline asm
	add.f64 	%fd84, %fd98, %fd83;
	// inline asm
	mov.b64 {%r325,%r326}, %fd84;
	// inline asm
	// inline asm
	shfl.idx.b32 %r327, %r325, %r12, %r370;
	// inline asm
	// inline asm
	shfl.idx.b32 %r331, %r326, %r12, %r370;
	// inline asm
	// inline asm
	mov.b64 %fd85, {%r327,%r331};
	// inline asm
	add.f64 	%fd86, %fd84, %fd85;
	// inline asm
	mov.b64 {%r337,%r338}, %fd86;
	// inline asm
	// inline asm
	shfl.idx.b32 %r339, %r337, %r13, %r370;
	// inline asm
	// inline asm
	shfl.idx.b32 %r343, %r338, %r13, %r370;
	// inline asm
	// inline asm
	mov.b64 %fd87, {%r339,%r343};
	// inline asm
	add.f64 	%fd88, %fd86, %fd87;
	// inline asm
	mov.b64 {%r349,%r350}, %fd88;
	// inline asm
	// inline asm
	shfl.idx.b32 %r351, %r349, %r14, %r370;
	// inline asm
	// inline asm
	shfl.idx.b32 %r355, %r350, %r14, %r370;
	// inline asm
	// inline asm
	mov.b64 %fd89, {%r351,%r355};
	// inline asm
	add.f64 	%fd90, %fd88, %fd89;
	// inline asm
	mov.b64 {%r361,%r362}, %fd90;
	// inline asm
	// inline asm
	shfl.idx.b32 %r363, %r361, %r15, %r370;
	// inline asm
	// inline asm
	shfl.idx.b32 %r367, %r362, %r15, %r370;
	// inline asm
	// inline asm
	mov.b64 %fd91, {%r363,%r367};
	// inline asm
	add.f64 	%fd19, %fd90, %fd91;
	setp.ne.s32	%p13, %r10, 0;
	@%p13 bra 	BB1_23;

	ld.global.u64 	%rd65, [%rd28];

BB1_22:
	mov.u64 	%rd32, %rd65;
	mov.b64 	 %fd92, %rd32;
	add.f64 	%fd93, %fd19, %fd92;
	mov.b64 	 %rd57, %fd93;
	atom.global.cas.b64 	%rd65, [%rd28], %rd32, %rd57;
	setp.ne.s64	%p14, %rd32, %rd65;
	@%p14 bra 	BB1_22;

BB1_23:
	add.s64 	%rd63, %rd63, 1;
	setp.lt.s64	%p15, %rd63, %rd3;
	@%p15 bra 	BB1_18;

BB1_24:
	ret;
}


