{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/testuser/GPUEnabler/Resources/gpu-enabler_2.11-1.0.0.jar\n",
      "Finished download of gpu-enabler_2.11-1.0.0.jar\n",
      "Starting download from file:///home/testuser/GPUEnabler/Resources/gpu-enabler-examples_2.11-1.0.0.jar\n",
      "Finished download of gpu-enabler-examples_2.11-1.0.0.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/testuser/GPUEnabler/Resources/gpu-enabler_2.11-1.0.0.jar\n",
    "%AddJar file:///home/testuser/GPUEnabler/Resources/gpu-enabler-examples_2.11-1.0.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.util.Random\n",
    "import com.ibm.gpuenabler.CUDADSImplicits._\n",
    "import com.ibm.gpuenabler._\n",
    "import org.apache.spark.sql.{Dataset, DataFrame, SparkSession}\n",
    "import scala.math._\n",
    "import scala.collection.mutable\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40g"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//spark.conf.set(\"spark.executor.memory\", \"20g\")\n",
    "//spark.conf.set(\"spark.driver.memory\", \"20g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DataPointKMeans\n",
       "defined class ClusterIndexes\n",
       "defined class Results\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class DataPointKMeans(features: Array[Double])\n",
    "case class ClusterIndexes(features: Array[Double], index: Int)\n",
    "case class Results(s0: Array[Int], s1: Array[Double], s2: Array[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession = org.apache.spark.sql.SparkSession@358a7c68\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "getDataSet: (spark: org.apache.spark.sql.SparkSession, inputPath: String)org.apache.spark.sql.Dataset[DataPointKMeans]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://172.17.0.2:4041)\" target=\"new_tab\">Spark UI: local-1500043706446</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1500043706446: Some(http://172.17.0.2:4041)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "//import sqlC.implicits._\n",
    "\n",
    "val sparkSession = SparkSession.builder.getOrCreate()\n",
    "import sparkSession.implicits._\n",
    "\n",
    "\n",
    "def getDataSet(spark: SparkSession, inputPath: String): Dataset[DataPointKMeans] = {\n",
    "\n",
    "val sparkSession = SparkSession.builder.getOrCreate()\n",
    "import sparkSession.implicits._\n",
    "    \n",
    "    val rawinputDF = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").csv(inputPath)\n",
    "\n",
    "    val pointsCached = rawinputDF.map(x=> {\n",
    "      val rowElem = x.getString(0).trim.split(\" \")\n",
    "      val len = rowElem.length\n",
    "      val buffer = new mutable.ListBuffer[Double]()\n",
    "      (0 until len).foreach { idx =>\n",
    "        if(!rowElem(idx).isEmpty)\n",
    "          buffer += rowElem(idx).toDouble\n",
    "      }\n",
    "\n",
    "      DataPointKMeans(buffer.toArray)\n",
    "    })\n",
    "\n",
    "    pointsCached\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Strfeatures: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n",
      "2351880\n",
      "2351880                                                                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = SparkSession.builder.getOrCreate()\n",
    "import sparkSession.implicits._\n",
    "\n",
    "def getDataFrame(spark: SparkSession, path: String): DataFrame = {\n",
    " // import spark.implicits._\n",
    "\n",
    "    val rawinputDF: DataFrame = spark.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(path)\n",
    "\n",
    "    rawinputDF\n",
    "}\n",
    "  \n",
    "  val pointsCached = getDataFrame(spark, \"./data/rawimageX3\").cache\n",
    "\n",
    "\n",
    "  \n",
    "  val toDouble = udf[Array[Double], String]( x => {\n",
    "      val rowElem = x.split(\" \")\n",
    "      val len = rowElem.length\n",
    "      val buffer = new mutable.ListBuffer[Double]()\n",
    "      (0 until len). foreach { idx =>\n",
    "        if(!rowElem(idx).isEmpty)\n",
    "          buffer += rowElem(idx).toDouble\n",
    "      }\n",
    "\n",
    "      (buffer.toArray)})\n",
    "      \n",
    "  // val schema = StructType( StructField(fieldName, StringType, nullable = true))\n",
    "  val data1 = pointsCached.toDF(\"Strfeatures\")\n",
    "  data1.printSchema\n",
    "\n",
    "\n",
    "  val data = data1.withColumn(\"features\", toDouble(data1(\"Strfeatures\"))).select(\"features\")// .as[DataPointKMeans]\n",
    "  data.printSchema\n",
    "  println(data.count())\n",
    "  // println(data.head.features.length)\n",
    "val means = data.rdd\n",
    "println(means.count())\n",
    "\n",
    "//data.cacheGpu(true)\n",
    "//data.loadGpu()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 17 in stage 10.0 failed 1 times, most recent failure: Lost task 17.0 in stage 10.0 (TID 107, localhost, executor driver): java.lang.ClassCastException: $line19.$read$$iw$$iw cannot be cast to $line19.$read$$iw$$iw\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1762)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1762)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n",
       "  ... 50 elided\n",
       "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1762)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val means = data.rdd\n",
    "println(means.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputPath = ./data/rawimageX3\n",
       "k = 16\n",
       "iter = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputPath=\"./data/rawimageX3\"\n",
    "val k = 16\n",
    "val iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2:==================================>                     (16 + 10) / 26]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pointsCached = [features: array<double>]\n",
       "data = [features: array<double>]\n",
       "N = 2351880\n",
       "d = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pointsCached = getDataSet(spark, inputPath).cache\n",
    "val data = pointsCached\n",
    "val N = pointsCached.count()\n",
    "val d = data.head.features.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pointsCached = getDataFrame(spark, inputPath).cache\n",
    "val data = pointsCached\n",
    "val N = pointsCached.count()\n",
    "//val d = data.head.features.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawinputDF = spark.read.option(\"header\", \"false\").option(\"inferSchema\", \"true\").csv(inputPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._\n",
    "\n",
    "val pointsCached = rawinputDF.map(in => {\n",
    "    val buffer = new mutable.ListBuffer[Double]()\n",
    "    in.getString(0).trim.split(\" \").foreach(x=>if (!x.isEmpty) buffer += x.toDouble)\n",
    "    (buffer.toArray)\n",
    "}).cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val data = pointsCached.toDF(\"features\").as[DataPointKMeans]\n",
    "val N = pointsCached.count()\n",
    "val d = data.head.features.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 3 ; N : 2351880; k : 16\n"
     ]
    }
   ],
   "source": [
    "println(s\"d: ${d} ; N : ${N}; k : ${k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epsilon = 0.5\n",
       "changed = true\n",
       "iteration = 0\n",
       "cost = 0.0\n",
       "ptxURL = /data/SparkGPUKmeans.ptx\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/SparkGPUKmeans.ptx"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val epsilon = 0.5\n",
    "var changed = true\n",
    "var iteration = 0\n",
    "var cost = 0.0\n",
    "val ptxURL = \"/data/SparkGPUKmeans.ptx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "centroidFn = DSCUDAFunction(getClusterCentroids,WrappedArray(features),WrappedArray(index),/data/SparkGPUKmeans.ptx,None,None,None)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DSCUDAFunction(getClusterCentroids,WrappedArray(features),WrappedArray(index),/data/SparkGPUKmeans.ptx,None,None,None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val centroidFn = DSCUDAFunction(\n",
    "        \"getClusterCentroids\",\n",
    "        Array(\"features\"),\n",
    "        Array(\"index\"),\n",
    "        ptxURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dimensions1 = > (Int, Int, Int, Int, Int, Int) = <function2>\n",
       "gpuParams1 = gpuParameters(<function2>,None)\n",
       "interFn = DSCUDAFunction(calculateIntermediates,WrappedArray(features, index),WrappedArray(s0, s1, s2),/data/SparkGPUKmeans.ptx,Some(<function1>),Some(gpuParameters(<function2>,None)),Some(450))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DSCUDAFunction(calculateIntermediates,WrappedArray(features, index),WrappedArray(s0, s1, s2),/data/SparkGPUKmeans.ptx,Some(<function1>),Some(gpuParameters(<function2>,None)),Some(450))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dimensions1 = (size: Long, stage: Int) => stage match {\n",
    "  case 0 => (450, 3, 1, 16, 1, 1)\n",
    "}\n",
    "\n",
    "val gpuParams1 = gpuParameters(dimensions1)\n",
    "\n",
    "val interFn = DSCUDAFunction(\n",
    "    \"calculateIntermediates\",\n",
    "    Array(\"features\", \"index\"),\n",
    "    Array(\"s0\", \"s1\", \"s2\"),\n",
    "    ptxURL,\n",
    "    Some((size: Long) => 1),\n",
    "    Some(gpuParams1), outputSize=Some(450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dimensions2 = > (Int, Int, Int, Int, Int, Int) = <function2>\n",
       "gpuParams2 = gpuParameters(<function2>,None)\n",
       "sumFn = DSCUDAFunction(calculateFinal,WrappedArray(s0, s1, s2),WrappedArray(s0, s1, s2),/data/SparkGPUKmeans.ptx,Some(<function1>),Some(gpuParameters(<function2>,None)),Some(1))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DSCUDAFunction(calculateFinal,WrappedArray(s0, s1, s2),WrappedArray(s0, s1, s2),/data/SparkGPUKmeans.ptx,Some(<function1>),Some(gpuParameters(<function2>,None)),Some(1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dimensions2 = (size: Long, stage: Int) => stage match {\n",
    "      case 0 => (1, 3, 1, 16, 1, 1)\n",
    "    }\n",
    "\n",
    "val gpuParams2 = gpuParameters(dimensions2)\n",
    "\n",
    "val sumFn = DSCUDAFunction(\n",
    "    \"calculateFinal\",\n",
    "    Array(\"s0\", \"s1\", \"s2\"),\n",
    "    Array(\"s0\", \"s1\", \"s2\"),\n",
    "    ptxURL,\n",
    "    Some((size: Long) => 1),\n",
    "    Some(gpuParams2), outputSize=Some(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addArr: (lhs: Array[Double], rhs: Array[Double])Array[Double]\n",
       "addArrI: (lhs: Array[Int], rhs: Array[Int])Array[Int]\n",
       "func1: (p: DataPointKMeans)ClusterIndexes\n",
       "func2: (c: ClusterIndexes)Results\n",
       "func3: (r1: Results, r2: Results)Results\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def addArr(lhs: Array[Double], rhs: Array[Double]) = {\n",
    "require(lhs.length == rhs.length, \"equal lengths\")\n",
    "\n",
    "lhs.zip(rhs).map { case (x, y) => x + y }\n",
    "}\n",
    "\n",
    "def addArrI(lhs: Array[Int], rhs: Array[Int]) = {\n",
    "require(lhs.length == rhs.length, \"equal lengths\")\n",
    "\n",
    "lhs.zip(rhs).map { case (x, y) => x + y }\n",
    "}\n",
    "def func1(p: DataPointKMeans): ClusterIndexes = {\n",
    "  ClusterIndexes(p.features, 0)\n",
    "}\n",
    "\n",
    "def func2(c: ClusterIndexes): Results = {\n",
    "  Results(Array.empty, Array.empty, Array.empty)\n",
    "}\n",
    "\n",
    "def func3(r1: Results, r2: Results): Results = {\n",
    "  Results(addArrI(r1.s0, r2.s0),\n",
    "    addArr(r1.s1, r2.s1),addArr(r1.s2, r2.s2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                        (0 + 26) / 26]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkSession = org.apache.spark.sql.SparkSession@358a7c68\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2351880"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = SparkSession.builder.getOrCreate()\n",
    "import sparkSession.implicits._\n",
    "data.cacheGpu(true)\n",
    "data.loadGpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "means = Array(DataPointKMeans([D@1e4d778e), DataPointKMeans([D@6fb525c4), DataPointKMeans([D@2fac7f94), DataPointKMeans([D@2cea7dfb), DataPointKMeans([D@59ee9eef), DataPointKMeans([D@1fb30b13), DataPointKMeans([D@532f97ae), DataPointKMeans([D@2cd53d95), DataPointKMeans([D@1e1d706), DataPointKMeans([D@60dbfba), DataPointKMeans([D@29b0da32), DataPointKMeans([D@2c8c6304), DataPointKMeans([D@235ac874), DataPointKMeans([D@484153ca), DataPointKMeans([D@670f61ba), DataPointKMeans([D@2854aa1c))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DataPointKMeans([D@1e4d778e), DataPointKMeans([D@6fb525c4), DataPointKMeans([D@2fac7f94), DataPointKMeans([D@2cea7dfb), DataPointKMeans([D@59ee9eef), DataPointKMeans([D@1fb30b13), DataPointKMeans([D@532f97ae), DataPointKMeans([D@2cd53d95), DataPointKMeans([D@1e1d706), DataPointKMeans([D@60dbfba), DataPointKMeans([D@29b0da32), DataPointKMeans([D@2c8c6304), DataPointKMeans([D@235ac874), DataPointKMeans([D@484153ca), DataPointKMeans([D@670f61ba), DataPointKMeans([D@2854aa1c)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val means = data.rdd.takeSample(true, k, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
